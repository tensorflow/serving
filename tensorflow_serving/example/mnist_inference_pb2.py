# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: mnist_inference.proto

import sys
_b = sys.version_info[0] < 3 and (lambda x: x) or (lambda x: x.encode('latin1'))
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
    name='mnist_inference.proto',
    package='tensorflow.serving',
    syntax='proto3',
    serialized_pb=
    _b('\n\x15mnist_inference.proto\x12\x12tensorflow.serving\"&\n\x0cMnistRequest\x12\x16\n\nimage_data\x18\x01 \x03(\x02\x42\x02\x10\x01\"\"\n\rMnistResponse\x12\x11\n\x05value\x18\x01 \x03(\x02\x42\x02\x10\x01\x32_\n\x0cMnistService\x12O\n\x08\x43lassify\x12 .tensorflow.serving.MnistRequest\x1a!.tensorflow.serving.MnistResponseb\x06proto3'))
_sym_db.RegisterFileDescriptor(DESCRIPTOR)




_MNISTREQUEST = _descriptor.Descriptor(
    name='MnistRequest',
    full_name='tensorflow.serving.MnistRequest',
    filename=None,
    file=DESCRIPTOR,
    containing_type=None,
    fields=[
    _descriptor.FieldDescriptor(
      name='image_data', full_name='tensorflow.serving.MnistRequest.image_data', index=0,
      number=1, type=2, cpp_type=6, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b('\020\001'))),
    ],
    extensions=[
    ],
    nested_types=[],
    enum_types=[
    ],
    options=None,
    is_extendable=False,
    syntax='proto3',
    extension_ranges=[],
    oneofs=[
    ],
    serialized_start=45,
    serialized_end=83,
)


_MNISTRESPONSE = _descriptor.Descriptor(
    name='MnistResponse',
    full_name='tensorflow.serving.MnistResponse',
    filename=None,
    file=DESCRIPTOR,
    containing_type=None,
    fields=[
    _descriptor.FieldDescriptor(
      name='value', full_name='tensorflow.serving.MnistResponse.value', index=0,
      number=1, type=2, cpp_type=6, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b('\020\001'))),
    ],
    extensions=[
    ],
    nested_types=[],
    enum_types=[
    ],
    options=None,
    is_extendable=False,
    syntax='proto3',
    extension_ranges=[],
    oneofs=[
    ],
    serialized_start=85,
    serialized_end=119,
)

DESCRIPTOR.message_types_by_name['MnistRequest'] = _MNISTREQUEST
DESCRIPTOR.message_types_by_name['MnistResponse'] = _MNISTRESPONSE

MnistRequest = _reflection.GeneratedProtocolMessageType('MnistRequest', (_message.Message,), dict(
  DESCRIPTOR = _MNISTREQUEST,
  __module__ = 'mnist_inference_pb2'
  # @@protoc_insertion_point(class_scope:tensorflow.serving.MnistRequest)
  ))
_sym_db.RegisterMessage(MnistRequest)

MnistResponse = _reflection.GeneratedProtocolMessageType('MnistResponse', (_message.Message,), dict(
  DESCRIPTOR = _MNISTRESPONSE,
  __module__ = 'mnist_inference_pb2'
  # @@protoc_insertion_point(class_scope:tensorflow.serving.MnistResponse)
  ))
_sym_db.RegisterMessage(MnistResponse)


_MNISTREQUEST.fields_by_name['image_data'].has_options = True
_MNISTREQUEST.fields_by_name['image_data']._options = _descriptor._ParseOptions(
    descriptor_pb2.FieldOptions(), _b('\020\001'))
_MNISTRESPONSE.fields_by_name['value'].has_options = True
_MNISTRESPONSE.fields_by_name['value']._options = _descriptor._ParseOptions(
    descriptor_pb2.FieldOptions(), _b('\020\001'))
import abc
from grpc.beta import implementations as beta_implementations
from grpc.framework.common import cardinality
from grpc.framework.interfaces.face import utilities as face_utilities

class BetaMnistServiceServicer(object):
  """<fill me in later!>"""
  __metaclass__ = abc.ABCMeta
  @abc.abstractmethod
  def Classify(self, request, context):
    raise NotImplementedError()

class BetaMnistServiceStub(object):
  """The interface to which stubs will conform."""
  __metaclass__ = abc.ABCMeta
  @abc.abstractmethod
  def Classify(self, request, timeout):
    raise NotImplementedError()
  Classify.future = None

def beta_create_MnistService_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):
  import mnist_inference_pb2
  import mnist_inference_pb2
  request_deserializers = {
    ('tensorflow.serving.MnistService', 'Classify'): mnist_inference_pb2.MnistRequest.FromString,
  }
  response_serializers = {
    ('tensorflow.serving.MnistService', 'Classify'): mnist_inference_pb2.MnistResponse.SerializeToString,
  }
  method_implementations = {
    ('tensorflow.serving.MnistService', 'Classify'): face_utilities.unary_unary_inline(servicer.Classify),
  }
  server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)
  return beta_implementations.server(method_implementations, options=server_options)

def beta_create_MnistService_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):
  import mnist_inference_pb2
  import mnist_inference_pb2
  request_serializers = {
    ('tensorflow.serving.MnistService', 'Classify'): mnist_inference_pb2.MnistRequest.SerializeToString,
  }
  response_deserializers = {
    ('tensorflow.serving.MnistService', 'Classify'): mnist_inference_pb2.MnistResponse.FromString,
  }
  cardinalities = {
    'Classify': cardinality.Cardinality.UNARY_UNARY,
  }
  stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)
  return beta_implementations.dynamic_stub(channel, 'tensorflow.serving.MnistService', cardinalities, options=stub_options)
# @@protoc_insertion_point(module_scope)
