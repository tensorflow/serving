ARG TF_SERVING_VERSION=latest
ARG TF_SERVING_BUILD_IMAGE=tensorflow/serving:${TF_SERVING_VERSION}-devel-mkl

FROM ${TF_SERVING_BUILD_IMAGE} as build_image
FROM ubuntu:16.04

ARG TF_SERVING_VERSION_GIT_BRANCH=master
ARG TF_SERVING_VERSION_GIT_COMMIT=head

LABEL maintainer="gvasudevan@google.com"
LABEL tensorflow_serving_github_branchtag=${TF_SERVING_VERSION_GIT_BRANCH}
LABEL tensorflow_serving_github_commit=${TF_SERVING_VERSION_GIT_COMMIT}

RUN apt-get update && apt-get install -y --no-install-recommends \
        ca-certificates \
        && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install TF Serving pkg
COPY --from=build_image /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server

# Install MKL libraries
COPY --from=build_image /usr/local/lib/libiomp5.so /usr/local/lib
COPY --from=build_image /usr/local/lib/libmklml_gnu.so /usr/local/lib
COPY --from=build_image /usr/local/lib/libmklml_intel.so /usr/local/lib

ENV LIBRARY_PATH '/usr/local/lib:$LIBRARY_PATH'
ENV LD_LIBRARY_PATH '/usr/local/lib:$LD_LIBRARY_PATH'


# Expose ports
# gRPC
EXPOSE 8500

# REST
EXPOSE 8501

# Set where models should be stored in the container
ENV MODEL_BASE_PATH=/models
RUN mkdir -p ${MODEL_BASE_PATH}

# The only required piece is the model name in order to differentiate endpoints
ENV MODEL_NAME=model

# Create a script that runs the model server so we can use environment variables
# while also passing in arguments from the docker command line.
# Setting MKL environment variables gave better performance.
# https://www.tensorflow.org/guide/performance/overview
# Read about Tuning MKL for the best performance
# and also
# During our experiments, setting tensorflow_session_parallelism to
# half of physical cores on single socket gave optimal peformance with MKL
# Add export MKLDNN_VERBOSE=1 to the below script,
# to see MKL messages in the docker logs when you send predict request.
# NOTE: We don't gurantee same settings to give optimal peformance across all hardwares
# please tune variables as required.
RUN echo '#!/bin/bash \n\n\
CORES_PER_SOCKET=`lscpu | grep "Core(s) per socket" | cut -d':' -f2 | sed "s/ //g"` \n\
SOCKETS=`lscpu | grep "Socket(s)" | cut -d':' -f2 | sed "s/ //g"` \n\
TOTAL_PHYSICAL_CORES=$((${CORES_PER_SOCKET} * ${SOCKETS} )) \n\
export HALF_CORES=$(( ${TOTAL_PHYSICAL_CORES} / 2)) \n\
export OMP_NUM_THREADS=${TOTAL_PHYSICAL_CORES} \n\
export KMP_BLOCKTIME=1 \n\
export KMP_SETTINGS=1 \n\
export KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \n\
tensorflow_model_server --port=8500 --rest_api_port=8501 \
--tensorflow_session_parallelism=${HALF_CORES} \
--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \
"$@"' > /usr/bin/tf_serving_entrypoint.sh \
&& chmod +x /usr/bin/tf_serving_entrypoint.sh

ENTRYPOINT ["/usr/bin/tf_serving_entrypoint.sh"]
